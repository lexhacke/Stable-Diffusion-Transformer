{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z37WdK4JX3IW",
    "outputId": "9bc495ab-c3b7-49fe-f8ea-b01d36609847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in ./.local/lib/python3.10/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from kagglehub) (21.3)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from kagglehub) (5.4.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kagglehub) (2.25.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in ./.local/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/lib/python3/dist-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "# @title Config\n",
    "image_shape = (256, 256, 3)\n",
    "latent_shape = (32,32,4)\n",
    "batch_size = 32\n",
    "filters = [64,128,256]\n",
    "z_channels = 4\n",
    "depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4PXOEZ7zNJ-",
    "outputId": "fefd4022-559b-4814-de77-c776d6605248"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/nagasai524/mini-coco2014-dataset-for-image-captioning/versions/1\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prvwiAbWCfSW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import gc\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_FoqA6kC-jUS"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "image_path = kagglehub.dataset_download(\"awsaf49/coco-2017-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qG9y4BvUClG8"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for dirpath, _, filenames in os.walk(image_path):\n",
    "  for filename in filenames:\n",
    "    if filename.endswith(\"jpg\"):\n",
    "      name = os.path.join(dirpath, filename)\n",
    "      img = cv2.imread(name)\n",
    "      img = cv2.resize(img, (image_shape[0],image_shape[0]))\n",
    "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "      latent = np.array([img/127.5 - 1])\n",
    "      data.append(latent)\n",
    "\n",
    "print(len(data)) # 163957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "b0ZDL5-teEuE"
   },
   "outputs": [],
   "source": [
    "# @title Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    self.indices = np.arange(len(data))\n",
    "    np.random.shuffle(self.indices)\n",
    "    self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.indices)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return torch.tensor(self.data[self.indices[idx]][0], dtype=torch.float32)\n",
    "plt.imshow(CustomDataset(data)[0]/2+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Xr5EE4q3C-LP"
   },
   "outputs": [],
   "source": [
    "# @title ResAttn\n",
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self, in_c):\n",
    "    super().__init__()\n",
    "    self.norm = nn.GroupNorm(num_groups=32, num_channels=in_c, eps=1e-6, affine=True)\n",
    "    self.Q = nn.Conv2d(in_c, in_c, kernel_size=1, stride=1, padding=0)\n",
    "    self.K = nn.Conv2d(in_c, in_c, kernel_size=1, stride=1, padding=0)\n",
    "    self.V = nn.Conv2d(in_c, in_c, kernel_size=1, stride=1, padding=0)\n",
    "    self.proj = nn.Conv2d(in_c, in_c, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, c, h, w = x.shape\n",
    "    R = self.norm(x)\n",
    "    q, v, k = self.Q(R), self.V(R), self.K(R)\n",
    "    q, v, k = q.reshape(b, c, h*w), v.reshape(b, c, h*w), k.reshape(b, c, h*w)\n",
    "    q, v, k = q.permute(0, 2, 1), v, k\n",
    "    R = torch.bmm(q, k)\n",
    "    R = F.softmax(R, dim=2)\n",
    "    R = torch.bmm(v, R)\n",
    "    R = R.permute(0, 2, 1)\n",
    "    R = R.reshape(b, c, h, w)\n",
    "    return self.proj(R) + x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, in_c, out_c):\n",
    "    super().__init__()\n",
    "    self.reshape = False\n",
    "    if in_c != out_c:\n",
    "      self.reshape = True\n",
    "      self.conv_reshape = nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1)\n",
    "    self.norm1 = nn.GroupNorm(num_groups=32, num_channels=out_c, eps=1e-6, affine=True)\n",
    "    self.conv1 = nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1)\n",
    "    self.norm2 = nn.GroupNorm(num_groups=32, num_channels=out_c, eps=1e-6, affine=True)\n",
    "    self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.reshape:\n",
    "      x = self.conv_reshape(x)\n",
    "    res = x\n",
    "    x = self.norm1(x)\n",
    "    x = x * torch.sigmoid(x)\n",
    "    x = self.conv1(x)\n",
    "    x = self.norm2(x)\n",
    "    x = x * torch.sigmoid(x)\n",
    "    x = self.conv2(x)\n",
    "    x = x + res\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OkN4JqOEY5gg"
   },
   "outputs": [],
   "source": [
    "# @title VAE\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, img_shape, filters, attn_resolutions, z_channels, depth):\n",
    "    super().__init__()\n",
    "    self.z_channels = z_channels\n",
    "    self.out_shape = (z_channels, img_shape[1] // 2**len(filters), img_shape[2] // 2**len(filters))\n",
    "    self.conv_out = nn.Conv2d(filters[-1], z_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    self.prep = nn.Conv2d(img_shape[0], filters[0], kernel_size=3, stride=2, padding=1)\n",
    "    self.down = nn.ModuleList()\n",
    "\n",
    "    current_res = img_shape[-1]\n",
    "    for i in range(len(filters)-1):\n",
    "      current_res = current_res // 2\n",
    "      block = nn.ModuleList([ResBlock(filters[i], filters[i+1])])\n",
    "      for _ in range(depth-1):\n",
    "        block.append(ResBlock(filters[i+1], filters[i+1]))\n",
    "      if current_res in attn_resolutions:\n",
    "        block.append(SpatialAttention(filters[i+1]))\n",
    "      block.append(nn.Conv2d(filters[i+1], filters[i+1], kernel_size=3, stride=2, padding=1))\n",
    "      self.down.append(block)\n",
    "\n",
    "    self.mid = nn.Sequential(ResBlock(filters[-1], filters[-1]),\n",
    "                            SpatialAttention(filters[-1]),\n",
    "                            ResBlock(filters[-1], filters[-1]))\n",
    "\n",
    "    self.norm = nn.GroupNorm(num_groups=32, num_channels=filters[-1], eps=1e-6, affine=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = self.prep(x)\n",
    "    for block in self.down:\n",
    "      for layer in block:\n",
    "        x = layer(x)\n",
    "    x = self.mid(x)\n",
    "    x = self.norm(x)\n",
    "    x = self.conv_out(x)\n",
    "    return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, latent_shape, filters, attn_resolutions, depth):\n",
    "    super().__init__()\n",
    "    self.out_shape = (3, latent_shape[1]*2**len(filters), latent_shape[2]*2**len(filters))\n",
    "    self.conv_in = nn.Conv2d(latent_shape[0], filters[0], kernel_size=3, stride=1, padding=1)\n",
    "    self.mid = nn.Sequential(ResBlock(filters[0], filters[0]),\n",
    "                            SpatialAttention(filters[0]),\n",
    "                            SpatialAttention(filters[0]),\n",
    "                            ResBlock(filters[0], filters[0]))\n",
    "\n",
    "    self.up = nn.ModuleList()\n",
    "    current_res = latent_shape[-1]\n",
    "    for i in range(len(filters)-1):\n",
    "      current_res = current_res * 2\n",
    "      block = nn.ModuleList([ResBlock(filters[i], filters[i+1])])\n",
    "      for _ in range(depth-1):\n",
    "        block.append(ResBlock(filters[i+1], filters[i+1]))\n",
    "      if current_res in attn_resolutions:\n",
    "        block.append(SpatialAttention(filters[i+1]))\n",
    "        block.append(SpatialAttention(filters[i+1]))\n",
    "      block.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "      self.up.append(block)\n",
    "\n",
    "    self.norm = nn.GroupNorm(num_groups=32, num_channels=filters[-1], eps=1e-6, affine=True)\n",
    "    self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "    self.conv_out = nn.Conv2d(filters[-1], 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv_in(x)\n",
    "    x = self.mid(x)\n",
    "    for block in self.up:\n",
    "      for layer in block:\n",
    "        x = layer(x)\n",
    "    x = self.norm(x)\n",
    "    x = self.upsample(x)\n",
    "    x = self.conv_out(x)\n",
    "    x = F.tanh(x)\n",
    "    x = x.permute(0,2,3,1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OIBhFP8ZsoA4"
   },
   "outputs": [],
   "source": [
    "# @title Discriminator + Perceptual\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, img_shape, filters=[256,512]):\n",
    "    super().__init__()\n",
    "    module_list = [nn.Conv2d(img_shape[0], filters[0], kernel_size=3, stride=2, padding=1),\n",
    "                   nn.BatchNorm2d(filters[0]),\n",
    "                   nn.LeakyReLU(0.2)]\n",
    "    for i in range(1,len(filters)):\n",
    "      module_list += [nn.Conv2d(filters[i-1], filters[i], kernel_size=3, stride=2, padding=1),\n",
    "                      nn.BatchNorm2d(filters[i]),\n",
    "                      nn.LeakyReLU(0.2)]\n",
    "\n",
    "    self.convs = nn.Sequential(*module_list)\n",
    "    self.mlp = nn.Sequential(nn.Conv2d(filters[-1], 1, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = self.convs(x)\n",
    "    x = self.mlp(x)\n",
    "    return x\n",
    "\n",
    "class vgg_builder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(vgg_builder, self).__init__()\n",
    "    convs = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "    self.N_slices = 5\n",
    "    self.slices = nn.ModuleList(list(nn.Sequential() for _ in range(self.N_slices)))\n",
    "    for x in range(4):\n",
    "      self.slices[0].add_module(str(x), convs[x])\n",
    "    for x in range(4, 9):\n",
    "      self.slices[1].add_module(str(x), convs[x])\n",
    "    for x in range(9, 16):\n",
    "      self.slices[2].add_module(str(x), convs[x])\n",
    "    for x in range(16, 23):\n",
    "      self.slices[3].add_module(str(x), convs[x])\n",
    "    for x in range(23, 30):\n",
    "      self.slices[4].add_module(str(x), convs[x])\n",
    "    for param in self.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  def forward(self, x):\n",
    "    feat_map = []\n",
    "    x = self.slices[0](x)\n",
    "    feat_map.append(x)\n",
    "    x = self.slices[1](x)\n",
    "    feat_map.append(x)\n",
    "    x = self.slices[2](x)\n",
    "    feat_map.append(x)\n",
    "    x = self.slices[3](x)\n",
    "    feat_map.append(x)\n",
    "    x = self.slices[4](x)\n",
    "    feat_map.append(x)\n",
    "    return feat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAl3vG4rufL1"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder((3,256,256), [64,128,256], {}, 4, 2)\n",
    "decoder = Decoder((4,32,32), [64,128,256], {}, 2)\n",
    "D = Discriminator((3,256,256))\n",
    "\n",
    "vgg = vgg_builder()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#encoder.load_state_dict(torch.load(path + f\"safe_tensors_depth2_256/encoder3.pt\", map_location=torch.device('cuda')))\n",
    "#decoder.load_state_dict(torch.load(path + f\"safe_tensors_depth2_256/decoder3.pt\",map_location=torch.device('cuda')))\n",
    "#D.load_state_dict(torch.load(path + f\"safe_tensors_depth2_256/discriminator3.pt\",map_location=torch.device('cuda')))\n",
    "\n",
    "print(f\"encoder: {sum(p.numel() for p in encoder.parameters())/(262144):.3f}MB\")\n",
    "print(f\"decoder: {sum(p.numel() for p in decoder.parameters())/(262144):.3f}MB\")\n",
    "print(f\"Discriminator: {sum(p.numel() for p in D.parameters())/(262144):.3f}MB\")\n",
    "print(f\"VGG: {sum(p.numel() for p in vgg.parameters())/(262144):.3f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "aPHD1HKtiZnE",
    "outputId": "7f72fe7b-7a11-46ac-a6a7-322c6e1c1e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 mse: 0.120 KL: 0.027 gan: 0.425 a vae: 0.020 a disc: 0.605 \n",
      "epoch:2 mse: 0.153 KL: 0.025 gan: 1.184 a vae: 0.006 a disc: 0.497 \n",
      "epoch:3 mse: 0.115 KL: 0.034 gan: 0.360 a vae: 0.018 a disc: 0.614 \n",
      "epoch:4 mse: 0.120 KL: 0.034 gan: 0.551 a vae: 0.003 a disc: 0.576 =========="
     ]
    }
   ],
   "source": [
    "# @title Training Configurations\n",
    "class Trainer():\n",
    "  def __init__(self, encoder, decoder, D, vgg, losses, data_len, ema=3, a_disc=1, a_vae=1, a_KL=0.05):\n",
    "    self.vgg_schedule = None\n",
    "    self.ema = 2/(ema+1)\n",
    "    self.a_disc = a_disc\n",
    "    self.a_vae = a_vae\n",
    "    self.a_KL = a_KL\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.D = D\n",
    "    self.vgg = vgg\n",
    "    self.encoder_optimizer = torch.optim.Adam(self.encoder.parameters(),  lr=1e-4)\n",
    "    self.encoder_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.encoder_optimizer, T_max=50)\n",
    "    self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(),  lr=1e-4)\n",
    "    self.decoder_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.decoder_optimizer, T_max=50)\n",
    "    self.D_optimizer = torch.optim.Adam(self.D.parameters(),  lr=1e-4)\n",
    "    self.D_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.D_optimizer, T_max=50)\n",
    "    self.losses = losses\n",
    "    self.loss_vals = {loss:0 for loss in losses}\n",
    "    self.data_len = data_len\n",
    "    self.loss_record = []\n",
    "    self.epoch = 1\n",
    "    self.index = 1\n",
    "    self.device = torch.device(\"cuda\")\n",
    "\n",
    "    self.encoder.to(self.device)\n",
    "    self.decoder.to(self.device)\n",
    "    self.D.to(self.device)\n",
    "    self.vgg.to(self.device)\n",
    "\n",
    "  def train_step(self, x, with_mse=True, freeze_ae=False, freeze_disc=False):\n",
    "    self.index += 1\n",
    "    x = x.to(self.device)\n",
    "    with torch.no_grad():\n",
    "      x_hat = self.decoder(self.encoder(x))\n",
    "    disc_loss = F.relu(1. - self.D(x)).mean() + F.relu(1. + self.D(x_hat)).mean() # Hinge\n",
    "    self.D_optimizer.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    self.D_optimizer.step()\n",
    "    self.D_scheduler.step()\n",
    "\n",
    "    if not freeze_ae:\n",
    "      z = self.encoder(x)\n",
    "      x_hat = self.decoder(z)\n",
    "      mse = F.mse_loss(x_hat, x)\n",
    "      KL = 0.5 * (z.mean() ** 2)\n",
    "      vgg_real = self.vgg(torch.permute(x, (0,3,1,2)))\n",
    "      vgg_fake = self.vgg(torch.permute(x_hat, (0,3,1,2)))\n",
    "      vgg_loss = 0\n",
    "      for i in range(len(vgg_real)):\n",
    "        vgg_loss += F.mse_loss(vgg_real[i], vgg_fake[i]) # * self.vgg_schedule[i]\n",
    "\n",
    "      adv_loss = 0\n",
    "      if not freeze_disc:\n",
    "        adv_loss = -(self.D(self.decoder(self.encoder(x))).mean())\n",
    "\n",
    "      loss = mse * with_mse + self.a_KL* KL + vgg_loss + self.a_vae * adv_loss\n",
    "      self.encoder_optimizer.zero_grad()\n",
    "      self.decoder_optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.encoder_optimizer.step()\n",
    "      self.decoder_optimizer.step()\n",
    "      self.encoder_scheduler.step()\n",
    "      self.decoder_scheduler.step()\n",
    "\n",
    "    self.update_batch({\"mse\":mse.item() if not freeze_ae else 0, \"gan\":disc_loss.item(), \"vgg\":vgg_loss.item() if not freeze_ae else 0})\n",
    "\n",
    "  def update_batch(self, loss_vals):\n",
    "    clear_output(wait=True)\n",
    "    for record in self.loss_record:\n",
    "      print(record)\n",
    "    self.loss_vals = {loss:(1-self.ema)*self.loss_vals[loss] + self.ema*loss_vals[loss] for loss in self.losses}\n",
    "    print(f\"epoch:{self.epoch} \", end=\"\")\n",
    "    for loss in self.losses:\n",
    "      print(f\"{loss}: {self.loss_vals[loss]:.3f} \", end=\"\")\n",
    "    for _ in range(int(self.index * 10 / self.data_len)):\n",
    "      print(\"=\", end=\"\")\n",
    "    for _ in range(int(self.index * 10 / self.data_len),10):\n",
    "      print(\"-\", end=\"\")\n",
    "\n",
    "  def update_epoch(self):\n",
    "    self.index = 0\n",
    "    record = f\"epoch:{self.epoch} \"\n",
    "    for loss in self.losses:\n",
    "      record += f\"{loss}: {self.loss_vals[loss]:.3f} \"\n",
    "    self.loss_record.append(record)\n",
    "    self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWIqh_Vaq8ld"
   },
   "outputs": [],
   "source": [
    "# @title Setup Training\n",
    "dataset   = CustomDataset(data)\n",
    "loader    = DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=2,\n",
    "                      pin_memory=True)\n",
    "epochs = 5\n",
    "trainer = Trainer(encoder, decoder, D, vgg, [\"mse\", \"gan\", \"vgg\"], len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijk30FzDEb33",
    "outputId": "800b2c6c-d767-45c7-e828-7a8169de7e5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(1, epochs):\n",
    "  index = 0\n",
    "  for i, x in enumerate(loader):\n",
    "    trainer.train_step(x, with_mse=False, freeze_disc=(epoch==1 and i < 50000/batch_size))\n",
    "  trainer.update_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auLyb_VGzNKC"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), path + f\"encoder.pt\")\n",
    "torch.save(decoder.state_dict(), path + f\"decoder.pt\")\n",
    "torch.save(D.state_dict(), path + f\"discriminator.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
